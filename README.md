# GPT研究、源码简介与应用探讨
研究脉络和基础知识

## 1. 自然语言处理（NLP）及其基本分析方法

自然语言处理（NLP）是计算机科学、人工智能和语言学的交叉领域，旨在使计算机能够理解、解释和产生人类语言。NLP集合了多种技术，其目标是缩小人类语言和机器理解之间的鸿沟。基于NLP的应用范围广泛，包括语音识别、自动翻译、情绪分析、文本摘要和聊天机器人等。  

NLP的基本分析方法通常包括语言模型、句法分析和语义分析。语言模型用于预测句子中下一个单词的概率，它可以是统计模型也可以是基于深度学习的模型。句法分析则关注句子的结构，目标是识别单词的依赖关系，建立句子的语法结构树。语义分析则试图理解句子的含义，这涉及解析单词、短语和句子的意义及其相互关系。在深度学习取得突破之前，NLP主要依赖于语料库统计和规则引擎，但如今，使用深度神经网络模型，例如卷积神经网络（CNNs）和循环神经网络（RNNs），可以有效地处理复杂的NLP任务。  

以前的nlp：从识别物体开始。物体+操作 -> 推理能力 -> 符号标记 -> 语言；  

    从识别物体开始： 在认知科学和语言学的发展历史中，对环境中的物体进行识别是认知发展的最基本层次。对于婴儿而言，学习识别周围的物体是他们语言发展的起点。在NLP的早期研究中，也模仿这一过程，首先关注的是如何让计算机识别和理解单个词汇或短语中的实体，即物体。  

    物体+操作 -> 推理能力： 一旦能够识别物体，下一步就是理解这些物体如何通过各种动作和状态的变化相互作用。在人类发展中，这对应于婴儿通过与环境互动，学习原因和结果之间的关系，发展出初步的因果推理能力。在NLP中，这涉及到理解物体之间的关系，如动词描述的动作，以及状态的变化，进而能够从描述中推导出新的信息或结论。  

    推理能力 -> 符号标记： 有了基本的推理能力之后，人类开始使用符号（如单词）来代表具体的物体和概念。这些符号是抽象的，它们不仅可以用来标记个体对象，还可以表示类别、属性、动作和关系等。在NLP的发展中，计算机被教会使用这些符号来代表语言中的元素，并能够根据它们之间的组合和结构来理解更复杂的语句。  

    符号标记 -> 语言： 最后，这些符号被组织成复杂的语言结构，形成了人类的语言能力。语言不仅仅是符号的集合，它还包括了语法规则、句法结构和语义关系，这些都是沟通复杂思想和情感的基础。NLP领域中的研究者试图让计算机理解和生成这些复杂的语言结构，使得计算机可以与人类进行有效的沟通。  
    以前的nlp面对的问题：给我一句话，通过软件处理并且理解这句话。  

（为什么以前没做出通用智能？因为要理解这句话，首先需要**知识**才行）  

## 2. 统计学中的向量化特征和回归分析

在统计学中，向量化特征是指将特征转换成数值形式，使其能够在各种统计模型中使用。对于文本数据，这个过程通常涉及到将文本转换为一组数值表示，这可以通过独热编码、词袋模型或TF-IDF等方法完成。这些技术将单词、短语或句子转换为向量，其中每个维度代表语料库中的一个特定元素，例如一个单词。通过这种转换，我们可以利用数学和统计方法来处理文本数据。

回归方程是一种统计工具，用于分析一个或多个自变量（或特征）与因变量之间的关系。在自然语言处理中，回归分析可以用来预测文本数据中的某些特性，如情感得分或文档的相关性评价。线性回归是最基本的回归形式，它假设自变量和因变量之间存在线性关系。然而，在实际的NLP任务中，关系往往更复杂，可能需要使用多项式回归或其他形式的回归，如逻辑回归，特别是在分类任务中。

## 3. 词嵌入（Embedding）的概念与实现

词嵌入是一种将词汇表的单词转换为低维空间中的向量的技术，这些向量捕捉了单词间的语义和语法关系。与传统的离散表示不同，如独热编码，词嵌入能够提供更丰富的表现形式。在词嵌入空间中，语义上相似或相关的单词被映射到空间中的邻近点。

onehot-encoder

将特征变成可以处理的数据涉及到将单词、短语或者整个文档映射到实数向量。这个过程通常依赖于无监督学习方法，例如神经网络，这些网络通过训练大量文本数据学习单词的分布式表示。词嵌入模型可以捕捉多种语言现象，包括词语的同义性、反义性、上下位关系等。通过这种方式，我们能够将自然语言映射到数学可处理的形式，为高级NLP任务如文本分类、情感分析或机器翻译提供基础。

未来是否依旧如此？[https://mp.weixin.qq.com/s/NIMmHFEbScOiv9Wy35b9bg](https://mp.weixin.qq.com/s/NIMmHFEbScOiv9Wy35b9bg)

## 4. Word2Vec的原理及应用

Word2Vec是一种流行的词嵌入技术，由Google的研究团队开发。它使用浅层神经网络来学习单词的向量表示。Word2Vec有两种主要的训练架构：连续词袋（CBOW）和Skip-Gram。CBOW预测当前单词基于其上下文，而Skip-Gram则预测上下文基于当前单词。这两种方法都通过考虑单词的上下文信息，捕捉到了单词之间的关系。

Word2Vec的优势在于其高效的训练过程以及生成的高质量词向量。得到的词嵌入捕捉了大量关于单词的语法和语义信息，这对于解决诸如单词相似度测量、命名实体识别、情感分析等NLP任务至关重要。

## 5. 循环神经网络（RNN）、长短期记忆神经网络（LSTM）和序列到序列（Seq2Seq）

模型循环神经网络（RNN）是一种神经网络，特别适合处理序列数据，如文本或时间序列数据。RNN通过在其网络架构中使用循环，允许信息在序列的不同位置之间传递，这使得RNN能够考虑到序列的顺序信息。

长短期记忆网络（LSTM）是RNN的一种变体，它通过使用专门的门控机制解决了传统RNN在处理长序列时出现的梯度消失或梯度爆炸问题。LSTM能够学习长距离的依赖关系，因此非常适合处理长序列数据。
序列到序列（Seq2Seq）模型是一种使用RNNs或LSTMs来处理序列输入并产生序列输出的架构。它通常包括一个编码器和一个解码器，编码器处理输入序列，解码器生成输出序列。Seq2Seq模型广泛应用于机器翻译、文本摘要和问答系统。

RNN、LSTM和Seq2Seq模型的共同点在于它们都能处理序列数据，并在序列的不同时间步上共享参数。不同之处在于LSTM通过引入门控机制来解决RNN的梯度问题，而Seq2Seq结构则通过编码器和解码器来处理复杂的序列转换任务。

Encoder 中将一个可变长度的信号序列变为固定长度的向量表达，Decoder 将这个固定长度的向量变成可变长度的目标的信号序列

## 6. 注意力机制与Transformer模型

注意力机制是一种使模型能够在处理信息时，“注意”到序列的某些部分更多的技术。在NLP中，这通常意味着模型在生成输出时，可以根据需要集中注意力于输入序列的相关部分。注意力机制提高了模型对输入序列的关键信息处理能力，尤其在处理长序列或需要模型对输入进行精细化处理的任务中。
Transformer模型是一种基于注意力机制的架构，完全抛弃了传统的循环层，而是仅通过注意力层和前馈网络来处理序列数据。Transformer的核心是自注意力机制，它允许模型在所有位置同时计算表示，从而极大提高了计算效率和模型性能。自注意力机制使模型能够捕获序列内的长距离依赖，而无需像RNN和LSTM那样顺序处理数据。
Transformer模型由于其并行处理能力和效率，在许多NLP任务中都取得了突破性的结果。它是当前许多高级NLP技术的基础，包括BERT、GPT等预训练语言模型。注意力机制的引入也使得模型能够更好地理解和利用上下文信息，相比起RNN和LSTM，在处理长序列以及复杂的序列转换任务时表现更加出色。

**Attention is all you need**  
论文地址：  
[https://arxiv.org/pdf/1706.03762.pdf](https://arxiv.org/pdf/1706.03762.pdf)

## 7.why it works

不错的参考文章

[https://peterbloem.nl/blog/transformers](https://peterbloem.nl/blog/transformers)



# 源码之前

## 1.神经网络中的线性变换
nn.Linear在神经网络中是一个线性变换，通常也称为全连接层或稠密层（Dense）。其基本原理是对输入数据执行线性变换以产生输出，这个变换遵循以下数学公式：
这里，(x) 是输入数据，(A) 是层的权重矩阵，(b) 是偏置向量，(y) 是变换后的输出。权重矩阵(A)的维度通常是(输出维度, 输入维度)，它定义了从输入空间到输出空间的线性映射。偏置(b)是一个与输出维度大小相同的向量，它为变换添加了一个常数偏移。
在实现神经网络时，nn.Linear层自动创建权重和偏置参数，并在训练过程中学习这些参数。在 PyTorch 中，这可以通过简单地实例化nn.Linear类来完成：
```python
import torch.nn as nn
# 输入特征数量为 3，输出特征数量为 2
linear_layer = nn.Linear(in_features=3, out_features=2)

# 示例输入向量
input_vector = torch.tensor([x_1, x_2, x_3], dtype=torch.float32)

# 执行线性变换
output_vector = linear_layer(input_vector)

```
在这里，linear_layer包含了权重 (A) 和偏置 (b)，并且output_vector就是变换后的结果。在训练期间，权重和偏置会根据损失函数和反向传播算法自动调整。


多维张量乘法 [https://cloud.tencent.com/developer/ask/sof/107812945](https://cloud.tencent.com/developer/ask/sof/107812945)

## 2.Dropout、Normalization、softmaxdropout

### Dropout 
是一种在神经网络训练过程中用于防止过拟合的正则化技术。Dropout 作用于网络层的神经元, 随机地“丢弃”（即在前向传播过程中临时移除）一部分神经元，使得被丢弃的神经元在这次前向传播中不会对后续层做出贡献，也不会在反向传播中接收梯度更新。  

Dropout 0.1 的含义是，在每次前向传播时，每个神经元有 10% 的概率不被激活，或者说是被“丢弃”。这意味着网络的每一次迭代都会在略微不同的网络架构上训练。这样做的好处是可以有效地减少神经元之间复杂的共适应关系，因为每个神经元不能依赖于其他特定的神经元，从而被迫学习更加鲁棒的特征。  

具体到实施时，对于某个给定的层，实现 Dropout 0.1 很简单：在前向传播过程中，对该层的输出应用一个由 0 和 1 组成的二值掩码，其中 1 的概率为 0.9（即 1 - 0.1），0 的概率为 0.1。然后，这个掩码与该层的输出相乘，结果就是部分神经元的输出会变为 0。这样，网络就在每次训练中只考虑部分连接，从而减少过拟合的风险。在测试时，为了补偿训练时的丢弃，通常会使用全部的神经元，并将所有神经元的输出乘以保留概率（这里是 0.9），或者在训练时就进行比例缩放，以确保输出的期望值保持不变。  

举一个简单的例子，如果你有一个包含 100 个神经元的层，并应用了 Dropout 0.1，在每次训练迭代中大约会有 10 个随机选中的神经元不参与前向传播和反向传播的过程。这样可以使得模型不会过于依赖任何一个神经元，增加了模型的泛化能力。  

### 标准化
nn.LayerNorm和nn.BatchNorm都是用于规范化神经网络中的输入数据的方法，但它们有一些区别和相同之处。  

相同之处：
1. 规范化：两者都是用来规范化神经网络的输入数据，以便提高网络的训练效果和泛化能力。  
2. 归一化：它们都对数据进行归一化操作，使得数据在训练过程中具有相似的分布特征。  
3. 参数学习：两者都通过学习参数来自适应地调整数据的分布，以提高神经网络的性能。  

区别：  
1. 应用位置：nn.LayerNorm通常应用于RNN（循环神经网络）等序列模型中，对每个时间步的输入进行归一化；而nn.BatchNorm通常应用于卷积神经网络（CNN）中的每个通道上，对每个通道的输入进行归一化。  
2. 维度：nn.LayerNorm对每个样本的特征维度进行归一化，而nn.BatchNorm对每个通道的特征维度进行归一化。  
3. 计算方式：nn.LayerNorm使用样本均值和方差来归一化数据，而nn.BatchNorm使用批次的均值和方差来归一化数据。  
4. 学习方式：nn.LayerNorm的学习参数独立于批次大小，可以在小批次上进行训练，而nn.BatchNorm的学习参数依赖于批次大小，需要较大的批次大小才能获得较好的性能。  

总体而言，nn.LayerNorm适用于序列数据，而nn.BatchNorm适用于卷积网络。选择哪个方法取决于具体的任务和网络架构。  

### softmax

（待补充）  
将高维数据转化为一维概率输出，激活函数。  
relu  
tanh  


## 3.核心-(encoder-decoder)Transformer结构图

（论文里的图 略）

特别地，decoder也传入了input。这里的input也和decoder的输出一起参与后续norm、求和与线性变换。


## 4.结构中的QKV

在自注意力机制中，Q、K、V分别代表查询（Query）、键（Key）和值（Value）这三个向量。这些向量是输入数据经过不同的权重矩阵转换得到的，具体来说：

查询（Query）: Query向量是代表当前单词的表示，用于与其他所有单词（通过它们的Key）的相似度进行比较。这个比较决定了在注意力机制中每个单词对当前单词的重要性。  

键（Key）: Key向量是用来和Query进行匹配的，以计算每个单词相对于Query的重要性程度。Query和Key之间的匹配通常通过点积（dot product）来完成，其结果代表了相似性的度量。  

值（Value）: Value向量包含了我们实际想要加权的信息，一旦计算出Query和每个Key的相似度，就可以用这个相似度作为权重，来加权每个Value。这样得到的加权Value的组合，就形成了注意力机制的输出，用于下一步的处理。  

在多头自注意力机制中，输入数据会被转换成多组Q、K和V向量，每组都通过不同的权重矩阵进行转换，这样可以从不同的子空间捕获信息，增加模型的表达能力。每个头计算得到的注意力输出会被拼接或者加权求和，然后再次被变换以形成最终的输出。这样可以让模型在处理序列时考虑到不同的特征表示和序列位置的信息。  




# 源码代码仓库：

SelfAttention
Encoder
Decoder
TransformerBlock
Transformer


# 源码之后

## 1.应用？未来！[https://www.bilibili.com/video/BV1Yz4y187Fn](https://www.bilibili.com/video/BV1Yz4y187Fn)  陆奇的演讲

**（某种生产资料）成本的结构性变化——任何大的产业变更结构化的通常原因。**  
90年代 信息获取的成本。谷歌地图，10亿成本可以服务1亿人，也可以服务60亿人。边际成本降低。    
当今 模型即知识。模型的成本从边际走向固定的时候，知识和模型将无处不在。更广泛的数字化和模型化。   
科技公司就是在用信息更有效地转化资源。  
未来经济模式，会不会是利用模型更有效地转化资源？会不会从服务经济变成体验经济  

[https://news.miracleplus.com/feeds?tab=hot#](https://news.miracleplus.com/feeds?tab=hot#)  大模型日报

## 2.gpt进化历程

|版本|年份|内容|
|---|---|---|
|GPT1|2017-2018|预训练，针对自然语言，预训练可以达到单独训练的效果|
|GPT2|2019|迁移学习 Fine Tuning
|GPT3|2020.6|关键的一步：强泛化能力 只需提示即可泛化-零样本、少样本泛化的能力|
|codex copilot|2021.7|代码重要-与代码对齐后，大模型的逻辑分析能力显著增强|
|GPT3.5|2022|根本性的一步：指令微调（对齐）。针对对话写指令，3.5w条指令。|
|GPT4|2023.3|多模态。系统工程化。plugin生态化。|

## 3.gpt相关工作和模型息息相关。

高效训练、压缩信息来表达世界知识。  
泛化能力（涌现、子概念空间要非常清晰）（增加参数、增加token后，得到新的能力如推理、算术、解决某些任务的能力）  
推理能力  
对齐工程——和人的认知空间、代码、表格、语言对齐、和价值观对齐  
算力有效利用（万卡通信、并行）  
核心模态的token，把重要的模态token化  
参数扩展、小型化、本地化  

基础能力足够强，可操作性足够强，多维度扩展能力足够强。  
自然语言（人话），代码，蛋白质语言，芯片电路，声音，图像……  


新玩意：[ai pin](https://hu.ma.ne/aipin)


作者：氦核  
2023.12.11  
